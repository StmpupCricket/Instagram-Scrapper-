name: Instagram Video Scraper

on:
  schedule:
    # Run daily at 2 AM UTC (adjust cron as needed)
    - cron: '0 2 * * *'
  workflow_dispatch:  # Allow manual triggers
  push:
    paths:
      - 'scraper/**'
      - '.github/workflows/**'

jobs:
  scrape:
    runs-on: ubuntu-latest
    
    steps:
    - name: Checkout repository
      uses: actions/checkout@v4
      
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.11'
        
    - name: Install system dependencies for Playwright
      run: |
        sudo apt-get update
        sudo apt-get install -y libwoff1 libopus0 libwebp6 libwebpdemux2 libenchant-2-2 libgudev-1.0-0 libsecret-1-0 libhyphen0 libgdk-pixbuf2.0-0 libegl1 libgles2 libevent-2.1-7 libnotify4
        
    - name: Install Python dependencies
      run: |
        cd scraper
        pip install -r requirements.txt
        
    - name: Install Playwright browsers
      run: |
        cd scraper
        python -m playwright install chromium --with-deps
        
    - name: Run Instagram scraper
      run: |
        cd scraper
        python main.py
        
    - name: Check if data was collected
      id: check-data
      run: |
        if [ -f "data/videos.json" ] && [ -s "data/videos.json" ]; then
          echo "data_found=true" >> $GITHUB_OUTPUT
        else
          echo "data_found=false" >> $GITHUB_OUTPUT
        fi
        
    - name: Commit and push results
      if: steps.check-data.outputs.data_found == 'true'
      run: |
        git config --global user.name 'GitHub Actions'
        git config --global user.email 'actions@github.com'
        
        # Add and commit the data files
        git add data/
        git commit -m "ðŸ“¹ Update scraped Instagram videos [$(date +'%Y-%m-%d %H:%M')]" || echo "No changes to commit"
        
        # Push changes
        git push
        
    - name: Upload data as artifact
      if: always()
      uses: actions/upload-artifact@v4
      with:
        name: instagram-videos
        path: data/
        retention-days: 7
        
    - name: Create summary report
      if: always()
      run: |
        if [ -f "data/videos.json" ]; then
          count=$(python -c "import json; data=json.load(open('data/videos.json')); print(data.get('total_videos', 0))")
          echo "## Scraping Results ðŸ“Š" >> $GITHUB_STEP_SUMMARY
          echo "- **Profile**: @t20worldcup" >> $GITHUB_STEP_SUMMARY
          echo "- **Videos found**: $count" >> $GITHUB_STEP_SUMMARY
          echo "- **Timestamp**: $(date)" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "### ðŸ“¥ Download Artifacts" >> $GITHUB_STEP_SUMMARY
          echo "Check the 'Artifacts' section above to download the JSON and TXT files." >> $GITHUB_STEP_SUMMARY
        else
          echo "## âŒ Scraping Failed" >> $GITHUB_STEP_SUMMARY
          echo "No data was collected. Check the logs for errors." >> $GITHUB_STEP_SUMMARY
        fi
