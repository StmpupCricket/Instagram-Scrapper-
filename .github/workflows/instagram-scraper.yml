name: Instagram Video Scraper

on:
  schedule:
    # Run daily at 2 AM UTC
    - cron: '0 2 * * *'
  workflow_dispatch:  # Allow manual triggers
  push:
    paths:
      - 'scraper/**'
      - '.github/workflows/**'

jobs:
  scrape:
    runs-on: ubuntu-22.04
    timeout-minutes: 30
    
    steps:
    - name: Checkout repository
      uses: actions/checkout@v4
      
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.10'
        
    - name: Install system dependencies
      run: |
        sudo apt-get update -y
        sudo apt-get install -y \
          libnss3 \
          libxss1 \
          libasound2 \
          libatk-bridge2.0-0 \
          libgtk-3-0 \
          libgbm1
        
    - name: Install Python dependencies
      run: |
        pip install --upgrade pip
        pip install playwright==1.40.0
        pip install beautifulsoup4==4.12.2
        pip install requests==2.31.0
        
    - name: Install Playwright
      run: |
        python -m playwright install --with-deps chromium
        python -m playwright install-deps
        
    - name: Create directory structure
      run: |
        mkdir -p scraper
        
    - name: Create main.py script
      run: |
        cat > scraper/main.py << 'EOF'
import json
import os
import time
import random
from datetime import datetime
from playwright.sync_api import sync_playwright

def scrape_instagram_videos(username, max_scrolls=8):
    videos_data = {
        "profile": f"@{username}",
        "scraped_at": datetime.now().isoformat(),
        "total_videos": 0,
        "videos": []
    }
    
    user_agents = [
        'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
        'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
    ]
    
    with sync_playwright() as p:
        browser = p.chromium.launch(
            headless=True,
            args=[
                '--no-sandbox',
                '--disable-setuid-sandbox',
                '--disable-dev-shm-usage'
            ]
        )
        
        context = browser.new_context(
            viewport={'width': 1366, 'height': 768},
            user_agent=random.choice(user_agents)
        )
        
        page = context.new_page()
        
        try:
            profile_url = f"https://www.instagram.com/{username}/"
            print(f"Navigating to: {profile_url}")
            
            page.goto(profile_url, wait_until='domcontentloaded', timeout=30000)
            time.sleep(5)
            
            video_urls_set = set()
            scroll_attempts = 0
            
            while scroll_attempts < max_scrolls and len(video_urls_set) < 30:
                video_elements = page.locator('video').all()
                
                for video in video_elements:
                    try:
                        src = video.get_attribute('src')
                        if src and 'video' in src and src not in video_urls_set:
                            video_urls_set.add(src)
                            print(f"Found video {len(video_urls_set)}")
                    except:
                        continue
                
                page.evaluate("window.scrollBy(0, window.innerHeight * 1.5);")
                time.sleep(random.uniform(2, 4))
                scroll_attempts += 1
            
            for url in video_urls_set:
                videos_data["videos"].append({
                    "url": url,
                    "scraped_at": datetime.now().isoformat()
                })
            
            videos_data["total_videos"] = len(videos_data["videos"])
            
        except Exception as e:
            print(f"Error: {str(e)}")
        finally:
            browser.close()
    
    return videos_data

def main():
    INSTAGRAM_USERNAME = "t20worldcup"
    
    print("Starting Instagram Video Scraper")
    results = scrape_instagram_videos(INSTAGRAM_USERNAME, max_scrolls=8)
    
    os.makedirs("data", exist_ok=True)
    
    json_file = "data/videos.json"
    with open(json_file, 'w', encoding='utf-8') as f:
        json.dump(results, f, indent=2, ensure_ascii=False)
    
    if results["videos"]:
        txt_file = "data/video_links.txt"
        with open(txt_file, 'w', encoding='utf-8') as f:
            for video in results["videos"]:
                f.write(f"{video['url']}\n")
    
    print(f"Complete! Found {results['total_videos']} videos")

if __name__ == "__main__":
    main()
EOF
        
    - name: Run scraper
      run: |
        cd scraper
        python main.py
      continue-on-error: true
        
    - name: Create results summary
      run: |
        if [ -f "data/videos.json" ]; then
          echo "## ðŸ“Š Scraping Results" >> $GITHUB_STEP_SUMMARY
          echo "Profile: @t20worldcup" >> $GITHUB_STEP_SUMMARY
          
          count=$(python -c "
import json
try:
    with open('data/videos.json', 'r') as f:
        data = json.load(f)
    print(data.get('total_videos', 0))
except:
    print('0')
")
          echo "Videos found: $count" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "Files saved in 'data/' directory" >> $GITHUB_STEP_SUMMARY
        else
          echo "## âŒ No data collected" >> $GITHUB_STEP_SUMMARY
          echo "Check the logs above for errors." >> $GITHUB_STEP_SUMMARY
        fi
        
    - name: Upload artifacts
      if: always()
      uses: actions/upload-artifact@v4
      with:
        name: instagram-videos
        path: data/
        retention-days: 30
